#!/usr/bin/env th

require 'torch'
require 'cudnn'

require 'laia'
local Batcher = laia.RandomBatcher

-- local str2bool_table = {
--    ['true'] = true, ['false'] = false,
--    ['t'] = true, ['f'] = false,
--    ['True'] = true, ['False'] = false,
--    ['1'] = true, ['0'] = false,
--    ['TRUE'] = true, ['FALSE'] = false
-- }

local opts = require 'laia.DecodeOptions'
local opt = opts.parse(arg)

torch.manualSeed(opt.seed)
if opt.gpu >= 0 then
  require 'cutorch'
  require 'cunn'
  require 'cudnn'
  cutorch.manualSeed(opt.seed)
end

local model = torch.load(opt.model).model

if opt.gpu >= 0 then
  cutorch.setDevice(opt.gpu + 1) -- +1 because lua is 1-indexed
  model = model:cuda()
  cudnn.convert(model, cudnn)
else
  -- CPU
  model:float()
end

-- Load symbols
local symbols_table
if opt.symbols_table then
  _, _, symbols_table = laia.read_symbols_table(opt.symbols_table)
end

model:evaluate()

-- Compute width factor from model
if opt.width_factor then
  opt.width_factor = 1
  local maxpool = model:findModules('cudnn.SpatialMaxPooling')
  for n=1,#maxpool do
    opt.width_factor = opt.width_factor * maxpool[n].kW
  end
else
  opt.width_factor = 0
end
local dv = Batcher(opt)
dv:load(opt.data)
dv:epochReset()
local n = 0
for batch=1,dv:numSamples(),opt.batch_size do
  -- Prepare batch
  local batch_img, _, _, batch_ids = dv:next(opt.batch_size)
  if opt.gpu >= 0 then
    batch_img = batch_img:cuda()
  end
  -- Forward through network
  local output = model:forward(batch_img)
  local batch_decode = laia.framewise_decode(opt.batch_size, output)
  for i=1,opt.batch_size do
    n = n + 1
    -- Batch can contain more images
    if n > dv:numSamples() then
      break
    end
    io.write(string.format('%s', batch_ids[i]))
    for t=1, #batch_decode[i] do
      if opt.symbols_table ~= '' then
        -- Print symbols
        io.write(string.format(' %s', symbols_table[batch_decode[i][t]]))
      else
        -- Print id's
        io.write(string.format(' %d', batch_decode[i][t]))
      end
    end
    io.write('\n')
  end
end
